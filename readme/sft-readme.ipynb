{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df339f80-13bf-443f-8f15-57d9d39a036a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install transformers[torch]==4.55.0\n",
    "! pip install accelerate\n",
    "! pip install datasets\n",
    "! pip install trl==0.22.0\n",
    "! pip install tf-keras\n",
    "! pip install numpy==1.26.0\n",
    "! pip install huggingface-hub==0.35.0\n",
    "! pip install jinja2==3.1.0\n",
    "! pip install peft\n",
    "# ssh tunneling to view tensor board\n",
    "# ssh -L 6006:localhost:6006 ubuntu@129.159.45.31\n",
    "\n",
    "# Start tensorboard UI\n",
    "# tensorboard --logdir ./logs/test/ --host 127.0.0.1 --port 6006\n",
    "\n",
    "# Docs: https://huggingface.co/docs/trl/en/sft_trainer (version=V0.19.0)\n",
    "# https://www.datacamp.com/tutorial/fine-tuning-qwen3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6b374b-030b-4b30-932c-4090adf0f674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = '''You are a language learning evaluator assessing the complexity of an English sentence given its context.\n",
    "\n",
    "Rubric:\n",
    "1 (A1) – Very basic words and phrases; simple self-introduction; minimal grammar.\n",
    "2 (A2) – Simple sentences; familiar everyday expressions; limited range.\n",
    "3 (B1) – Can write or speak in connected sentences about familiar topics; some errors.\n",
    "4 (B2) – Generally fluent; can discuss abstract topics; good grammar control.\n",
    "5 (C1) – Flexible, natural use of language; few errors; advanced vocabulary.\n",
    "6 (C2) – Near-native mastery; precise, nuanced expression; fully natural flow.\n",
    "\n",
    "Please give a rating between 1-6 following the rubric above.\n",
    "'''\n",
    "\n",
    "PROMPT_TEMPLATE = '''\n",
    "\n",
    "Context: {{ context }}\n",
    "Sentence: {{ sentence }}\n",
    "Rating (1-6):\n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b27514-0641-4b81-98ec-bc7a05807fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(\"readme/readme_en_train.csv\")\n",
    "train_df = train_df.dropna(subset=[\"Rating\", \"Sentence\", \"Paragraph\"])\n",
    "dev_df = pd.read_csv(\"readme/readme_en_val.csv\")\n",
    "dev_df = dev_df.dropna(subset=[\"Rating\", \"Sentence\", \"Paragraph\"])\n",
    "test_df = pd.read_csv(\"readme/readme_en_test.csv\")\n",
    "test_df = test_df.dropna(subset=[\"Rating\", \"Sentence\", \"Paragraph\"])\n",
    "\n",
    "train = Dataset.from_pandas(train_df)\n",
    "dev = Dataset.from_pandas(dev_df)\n",
    "test = Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train,\n",
    "    \"validation\": dev,\n",
    "    \"test\": test\n",
    "})\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9d7141-f038-40a1-a7a0-768e5258f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from jinja2 import Template\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "JINJA_PROMPT_TEMPLATE = Template(PROMPT_TEMPLATE)\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "def preprocess(example):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": JINJA_PROMPT_TEMPLATE.render(\n",
    "            context=example['Paragraph'],\n",
    "            sentence=example['Sentence']\n",
    "        )},\n",
    "        {\"role\": \"assistant\", \"content\": f\"The score is {example['Rating']}\"}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(messages, add_generation_prompt=False)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "item = next(iter(dataset[\"train\"]))\n",
    "print(item['messages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc90fdf-536e-4669-b187-fd44bbe55135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "import transformers\n",
    "import torch\n",
    "import os, shutil\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"] # make sure to save the lm_head and embed_tokens as you train the special tokens\n",
    ")\n",
    "\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "                                            model_id,\n",
    "                                            torch_dtype=torch.bfloat16,\n",
    "                                            device_map=\"auto\",\n",
    "                                            attn_implementation=\"eager\")\n",
    "\n",
    "\n",
    "shutil.rmtree(\"models/readme/gemma_1b_lora/test\")\n",
    "\n",
    "def train():\n",
    "\n",
    "    torch_dtype = model.dtype\n",
    "\n",
    "    args = SFTConfig(\n",
    "        output_dir=\"models/readme/gemma_1b_lora/test\",              # directory to save and repository id\n",
    "        max_length=512,                         # max sequence length for model and packing of the dataset\n",
    "        packing=False,                          # Groups multiple samples in the dataset into a single sequence\n",
    "        num_train_epochs=10,                     # number of training epochs\n",
    "        per_device_train_batch_size=16,          # batch size per device during training\n",
    "        per_device_eval_batch_size=16,\n",
    "        gradient_checkpointing=False,           # Caching is incompatible with gradient checkpointing\n",
    "        optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "        logging_steps=10,                        # log every step\n",
    "        save_strategy=\"epoch\",                 # save checkpoint every epoch\n",
    "        eval_strategy=\"epoch\",                  # evaluate checkpoint every epoch\n",
    "        learning_rate=1e-05,            # learning rate\n",
    "        fp16=True if torch_dtype == torch.float16 else False,   # use float16 precision\n",
    "        bf16=True if torch_dtype == torch.bfloat16 else False,  # use bfloat16 precision\n",
    "        warmup_ratio=0.05,\n",
    "        lr_scheduler_type=\"linear\",           # use constant learning rate scheduler\n",
    "        push_to_hub=False,                       # push model to hub\n",
    "        report_to=\"tensorboard\",  # or \"wandb\", \"comet_ml\", etc.\n",
    "        logging_dir=\"./logs/gemma_1b_lora/test\",\n",
    "        # dataset_text_field=\"text\",\n",
    "        load_best_model_at_end=True,\n",
    "        save_total_limit=1,\n",
    "        dataset_kwargs={\n",
    "            \"add_special_tokens\": False, # Template with special tokens\n",
    "            \"append_concat_token\": True, # Add EOS token as separator token between examples\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = SFTTrainer(\n",
    "            model,\n",
    "            train_dataset=dataset[\"train\"],\n",
    "            eval_dataset=dataset[\"validation\"],\n",
    "            args=args,\n",
    "            processing_class=tokenizer,\n",
    "            # peft_config=peft_config,\n",
    "            # compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "# This will spawn multiple GPU processes directly from Jupyter:\n",
    "# notebook_launcher(train, num_processes=2)\n",
    "trainer = train()\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777170ce-6ab2-4478-9686-9a81ee6b0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "import tansformers\n",
    "\n",
    "model_id = \"google/gemma-3-1b-it\"\n",
    "\n",
    "# Load Model base model\n",
    "base_model = transformers.AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Merge LoRA and base model and save\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"models/readme/gemma/lora/checkpoint-141\")\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "merged_model.save_pretrained(\"models/readme/gemma/lora/merged\")\n",
    "\n",
    "processor = AutoTokenizer.from_pretrained(model_id)\n",
    "processor.save_pretrained(\"models/readme/gemma/lora/merged/merged\")\n",
    "\n",
    "model = merged_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598bea9c-9238-45f3-9160-df83bedc6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "! export CUDA_LAUNCH_BLOCKING=1\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "                    \"models/readme/gemma/test/checkpoint-141\",\n",
    "                    torch_dtype=torch.bfloat16,\n",
    "                    device_map=\"auto\",\n",
    "                    attn_implementation=\"flash_attention_2\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"models/readme/gemma/test/checkpoint-141\")\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def build_prompt(messages):\n",
    "    return pipe.tokenizer.apply_chat_template(\n",
    "        messages[:-1],\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "def extract_completion(s: str) -> str:\n",
    "    if \"The score is\" in s:\n",
    "        return s.split(\"The score is\", 1)[1].strip()\n",
    "    return s.strip()\n",
    "\n",
    "test_ds = dataset[\"test\"]\n",
    "\n",
    "# Precompute prompts + gold labels\n",
    "prompts = []\n",
    "golds = []\n",
    "for example in test_ds:\n",
    "    prompts.append(build_prompt(example[\"messages\"]))\n",
    "    golds.append(extract_completion(example[\"messages\"][-1][\"content\"]))\n",
    "\n",
    "# Choose a batch size based on GPU RAM\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "preds = []\n",
    "for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Evaluating\"):\n",
    "    batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "\n",
    "    outputs = pipe(\n",
    "        batch_prompts,\n",
    "        max_new_tokens=64,\n",
    "        do_sample=True,         # set False for deterministic accuracy\n",
    "        temperature=0.1,\n",
    "        top_k=10,\n",
    "        top_p=0.2,\n",
    "        eos_token_id=[tokenizer.eos_token_id,\n",
    "                      tokenizer.convert_tokens_to_ids(\"<end_of_turn>\")],\n",
    "        disable_compile=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "\n",
    "    for prompt, out in zip(batch_prompts, outputs):\n",
    "        gen = out[0][\"generated_text\"]\n",
    "        pred = extract_completion(gen[len(prompt):].strip())\n",
    "        preds.append(pred)\n",
    "\n",
    "# Compute accuracy\n",
    "acc = sum(p == g for p, g in zip(preds, golds))\n",
    "print(\"Accuracy:\", acc * 100.0 / len(golds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef375ac-5c0c-441b-8fcc-a5b69e22424a",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Gemma 270m (full finetune - lr 1e05) - 54.1\n",
    "Gemma 270m (full finetune - lr 1e05 - max grad norm - 1.0) - 54.1\n",
    "Gemma 270m (full finetune - lr 5e05 - max grad norm - 1.0) - 48.3\n",
    "Gemma 270m (full finetune - lr 5e06 - max grad norm - 1.0) - Stopped the run because the learning is too slow.\n",
    "Gemma 270m (lora - lr 1e04 - rank and alpha 16) - 51.35\n",
    "Gemma 270m (lora - lr 5e04 - rank and alpha 16) - 49.3\n",
    "Gemma 270m (lora - lr 5e05 - rank and alpha 16) - 45.6\n",
    "\n",
    "Gemma 1b (lora - 16, 32 lr 1e04, constant lr) - 50.68\n",
    "Gemma 1b (lora - 32, 64 lr 1e04, warmup linear) - 51.35\n",
    "Gemma 1b (lora - 16, 32 lr 1e04, warmup linear) - 52.03\n",
    "Gemma 1b (full finetune - lr 1e05, warmup linear) - 54.39\n",
    "\n",
    "Why did I have to put gradient checkpointing False?\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004710c4-9fdf-4e5e-ac9a-1e726fdbca1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def compute_metrics(eval_pred):\n",
    "\n",
    "#     def extract_completion(s: str) -> str:\n",
    "#         if \"The score is\" in s:\n",
    "#             return s.split(\"The score is\", 1)[1].strip()\n",
    "#         return s.strip()\n",
    "\n",
    "#     # eval_pred is usually an EvalPrediction\n",
    "#     logits = eval_pred.predictions\n",
    "#     labels = eval_pred.label_ids\n",
    "\n",
    "#     # Handle logits shape flexibly\n",
    "#     logits = np.array(logits)\n",
    "#     labels = np.array(labels)\n",
    "\n",
    "#     # If logits is (B, T, V) use argmax along -1:\n",
    "#     if logits.ndim == 3:\n",
    "#         preds = np.argmax(logits, axis=-1)\n",
    "\n",
    "#     # Print a few examples for debugging\n",
    "#     b = labels.shape[0]\n",
    "\n",
    "#     accuracy = []\n",
    "#     for i in range(b):\n",
    "#         gold_ids = labels[i]\n",
    "#         pred_ids = preds[i]\n",
    "\n",
    "#         # ignore masked positions (-100)\n",
    "#         gold_ids = gold_ids[gold_ids != -100]\n",
    "#         pred_ids = pred_ids[pred_ids != -100]\n",
    "\n",
    "#         gold_str = tokenizer.decode(gold_ids, skip_special_tokens=True)\n",
    "#         pred_str = tokenizer.decode(pred_ids, skip_special_tokens=True)\n",
    "\n",
    "#         gold_completion = int(float(extract_completion(gold_str)))\n",
    "#         pred_completion = 0.0\n",
    "#         try:\n",
    "#             pred_completion = int(float(extract_completion(pred_str)))\n",
    "#         except:\n",
    "#             pred_completion = 0.0\n",
    "\n",
    "#         accuracy.append(gold_completion == pred_completion)\n",
    "\n",
    "#     # IMPORTANT: must return a dict of numbers\n",
    "#     return {\"accuracy\": sum(accuracy) * 1.0 / len(accuracy)}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
